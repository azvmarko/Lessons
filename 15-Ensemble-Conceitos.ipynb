{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principais algoritmos simples de classificação para um Data Science:\n",
    "#### KNN (K-Nearest Neighbors)\n",
    "\n",
    "#### Logistic Regression\n",
    " - __[Artigo do Datacamp](https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python)__\n",
    " - __[Tutorial do Andrew Ng](https://www.youtube.com/watch?v=-la3q9d7AKQ)__\n",
    " - __[Tutorial StatsQuest](https://www.youtube.com/watch?v=yIYKR4sgzI8)__\n",
    "\n",
    "#### Naive Bayes\n",
    " - __[Artigo do Datacamp](https://www.datacamp.com/community/tutorials/naive-bayes-scikit-learn)__\n",
    " - __[Teoria de Probabilidade](https://www.youtube.com/watch?v=PrkiRVcrxOs)__\n",
    " - __[Video Tutorial](https://www.youtube.com/watch?v=CPqOCI0ahss&t=236s)__\n",
    "\n",
    "#### SVM (Support Vector Machine)\n",
    " - __[Artigo do Datacamp](https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python)__\n",
    " - __[Tutorial Codebasics](https://www.youtube.com/watch?v=FB5EdxAGxQg)__\n",
    "\n",
    "#### Decision Tree\n",
    " - __[Artigo Datacamp](https://www.datacamp.com/community/tutorials/decision-tree-classification-python)__\n",
    " - __[Tutorial StatQuest](https://datahackers.com.br/blog)__\n",
    " - __[Google Devs from zero](https://www.youtube.com/watch?v=LDRbO9a6XPU)__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensembles são agrupamentos de algoritmos treinados para a realização de uma mesma tarefa.\n",
    "\n",
    "Os ensembles podem ser homogênios, utilizando vários modelos gerados a partir do mesmo método (por exemplo, contendo apenas regressões lineares) ou heterogênios (utilizando vários tipos de modelos diferentes para chegar em um resultado). Todos eles partem do princípio que serão usados *weak learners* para atingir o objetivo final\n",
    "\n",
    "Existem essencialmente três tipos de ensembles:\n",
    "    \n",
    "    - Bagging\n",
    "    - Boosting\n",
    "    - Stacking\n",
    "    \n",
    "Para o *Bagging* e para a *Random Forest*, pode-se agregar o resultado de algumas maneiras diferentes: Fazendo uma média dos valores, quando se está referindo à resultados de valores contínuos; fazendo a moda dos valores (votação) quando se está falando de classificações  ou ainda atribuindo um peso a cada modelo e fazendo uma média ponderada dos seus resultados. Todos esses modelos podem ser feitos de forma paralela (ao mesmo tempo).\n",
    "\n",
    "Um exemplo de obtenção do resultado por votação:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/votacao.jpg\" align=\"center\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging ensemble \n",
    "\n",
    "A palavra *Bagging* é a abreviação de *Bootstrap Aggregation*, onde a primeira palavra indica a criação de um sample com reposição e a segunda palavra a agregação dos resultados obtidos por modelos treinados a partir dessas diferentes samples. Ou seja, primeiramente temos a etapa de *resampling* para cada modelo, para então fazermos seus treinos e por fim agregar suas respostas da maneira mais adequada.  \n",
    "\n",
    "Na criação das novas samples a partir da sample original, todos os exemplos da sample original, nesse caso, tem a mesma chance de aparecer na nova sample. Cada exemplo que é selecionado para entrar na nova sample sempre retorna ao \"banco de exemplos\" existentes na sample original, podendo assim ser escolhido novamente. As novas samples podem ser de tamahos menores ou do mesmo tamanho da sample original. \n",
    "\n",
    "<img src=\"img/Bagging_1.png\" align=\"center\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com todas as novas samples geradas, pode-se então treinar cada um dos seus respectivos modelos, como é ilustrado na imagem abaixo:\n",
    "<img src=\"img/Bagging_2.png\" align=\"center\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessa forma, formamos os várias *weak learners* (modelos que podem ser de quaisquer algoritmos como regressões logísticas, árvores de decisão, KNN; geralmente escolhe-se um deles, onde o mais comum é o de Árvore de Decisão pelo fato de ser mais facilmente interpretável), que irão gerar os seus \"chutes\" (previsões) para cada um dos exemplos de validação/teste, que serão agregados da forma mais adequada para cada tipo de problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T16:43:03.093085Z",
     "start_time": "2019-07-01T16:43:03.061836Z"
    }
   },
   "source": [
    "<img src=\"img/bagging_2.png\" align=\"center\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar bootstrap (criação de sample com reposição) para construir as samples faz com que a probabilidade de se usar na nova seleção um exemplo já escolhido anteriormente ou um ainda não existente na sample que está sendo construída seja exatamente a mesma, já que após cada seleção há o retorno do exemplo selecionado de volta para a \"bag\" de exemplos. Isso faz com que a variância dos modelos que serão treinados seja menor do que sem usar o boostrap. No final desse algoritmo, os resultados são agregados usando uma média simples. \n",
    "\n",
    "Pegando um exemplo de uma variável e verificando como fica a entropia e o ganho (greedy) dessa feature em relação à variável resposta em duas situações diferentes, resultados:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Exemplo_DT_1.png\" align=\"center\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Exemplo_DT_2.png\" align=\"center\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo *Random Forests* pode ser considerado uma evolução do método de *bagging*. Suas árvores podem ser geradas com ou sem resample dos examplos da base de dados~, e caso seja optado por não fazer o *bagging* com os exemplos, todas as árvores serão testadas com o mesmo sample. A novidade aqui está na escolha dos preditores (features, características...) da base de dados. O hiperparâmetro ou o número de preditores que cada árvore terá pode ser determinado dependendo do tipo de resultado que se pretende obter, sendo esta a sugestão do criador, mas também de diversos outras maneiras como usando o log:\n",
    "    - Quando uma regressão: teremos que o número de parâmetros de cada árvore será o número total de preditores sobre 3 (p/3)\n",
    "    - Quando uma classificação: teremos que o número de parâmetros de cada árvore será a raiz quadrada do número total de preditores (sqrt(p))\n",
    "    \n",
    "   \n",
    "A partir dessa determinação, cada árvore terá seus parâmetros selecionados randomicamente do total (por exemplo, se queremos resolver um problema de classificação e temos um total de 9 parâmetros, cada árvore terá 3 parâmetros, os quais serão selecionados aleatóriamente para cada uma). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine que estamos construindo um classificador. Ele terá um erro, correto? E se ao invés de treinar nossos modelos e cada um tivesse seu erro, nós treinássemos modelos especializados em corrigir os erros anteriores? Diferente dos ensembles anteriores, que são ensembles em paralelo, boosting é um ensemble sequencial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/boosting_0.png\" align=\"left\" width=\"20%\">\n",
    "1) Primeiramente, criamos um classificador base. Ele vai conter erros, e precisamos corrigí-los"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/boosting_1.png\" align=\"left\" width=\"20%\">\n",
    "2) Criamos um segundo classificador, mas que opera em cima dos erros do primeiro (wrong predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/boosting_2.jpeg\" align=\"left\" width=\"20%\">\n",
    "3) Continuamos esse loop até chegar na performance desejada (um threshold específico, acurácia,...). No momento de definir este *trashold* precisamos tomar cuidado para não overfitarmos os nossos dados. O output final é dado pela Weighted Average dos sub-modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/boosting_3.png\" align=\"left\" width=\"20%\">\n",
    "4) Nosso modelo final é uma combinação de todos os outros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Gradient Boosting e o XGBoost são os modelos de boosting mais utilizados por diversas razões, entretanto, entre os dois citados, o XGBoost costuma ter um desempenho melhor por ter uma regularização diferenciada. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmos baseados em boosting:\n",
    " - Adaboost\n",
    " - GBM (Gradient Boosting Machine)\n",
    " - XGBoost (Extreme Gradient Boost)\n",
    " - LightGBM\n",
    " - CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Stacking\n",
    "\n",
    "A ideia por trás do stacking se divide em 2 partes. Treinar diversos algoritmos nos dados, e que as visões deles sobre os algoritmos sejam complementares, ou seja, cada algoritmo aprenda algo a partir do anterior. Desse modo, o stacking se dá por 2 loops:\n",
    " - loop externo: treino de cada algoritmo de modo independente.\n",
    " - loop interno: dividir o dataset de treino em 10 partes, serão, portanto, 10 combinações de 9:1. Treinar cada algoritmo em cada uma dessas 10 combinações. Os outputs de cada algoritmo no dataset de teste servirão de features para o próximo algoritmo no loop externo\n",
    "De modo visual, o stacking se parece com isso:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T16:52:19.278469Z",
     "start_time": "2019-07-01T16:52:19.247220Z"
    }
   },
   "source": [
    "<img src=\"img/stacking_77.png\" align=\"center\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T16:52:40.482193Z",
     "start_time": "2019-07-01T16:52:40.450945Z"
    }
   },
   "source": [
    "Exemplo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/stacking_0.png\" align=\"left\" width=\"20%\">\n",
    "1) Após ter feito o `train_test_split`, quebrar o dataset de treino em 10 partes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/stacking_1.png\" align=\"left\" width=\"20%\"> \n",
    "2) Fazendo combinações de 9:1 partes, treinar o algoritmo (por ex, uma Decision Tree) nas 9 partes e usar a décima parte com pré-teste. Repitir isso para todas as combinações. Isso ajuda o algoritmo a entender pontos de vista diferentes sobre o dataset. O nome disso é **K-Fold Cross Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/stacking_2.png\" align=\"left\" width=\"20%\">\n",
    "3) Após o treino do primeiro algoritmo em seu loop interno, faça as predictions no dataset de teste. Esse output será utilizado como *feature* para os próximos algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/stacking_3.png\" align=\"left\" width=\"20%\">\n",
    "4) Repitir o passo desse loop externo com os próximos algoritmos, até alcançar a performance desejada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O bom do Stacking é que se pode criar as features que desejar com esse método! Por exemplo, antes de fazer um regressor para prever a quantidade de vendas de um determinado produto na próxima semana, pode-se antes fazer um classificador que preveja se as vendas irão aumentar ou diminuir, para então passar esse output como nova feature :D\n",
    "\n",
    "Algoritmos de Stacking:\n",
    "- quaisquer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/stacking_5.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse método é amplamente utilizado nas competições do Kaggle!\n",
    "\n",
    "De um modo geral, os métodos de ensemble citados acima diminuem a variância do modelo único, já que eles combinam as estimativas dos diversos modelos. Isso faz com que o resultado seja mais estável (faça uma analogia com a Sabedoria da multidão). Se o problema do modelo único for baixa performance, o bagging será capaz de dar um bias melhor mas provavelmente a abordagem de boosting fará isso melhor. Entretando, se o modelo singular está overfitando, o bagging se torna a abordagem mais interessante. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, alguns códigos base para guardar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Classifier\n",
    "model1 = tree.DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)\n",
    "\n",
    "pred1=model1.predict(x_test)\n",
    "pred2=model2.predict(x_test)\n",
    "pred3=model3.predict(x_test)\n",
    "\n",
    "final_pred = np.array([])\n",
    "for i in range(0,len(x_test)):\n",
    "    final_pred = np.append(final_pred, mode([pred1[i], pred2[i], pred3[i]]))\n",
    "    \n",
    "from sklearn.ensemble import VotingClassifier\n",
    "model1 = LogisticRegression(random_state=1)\n",
    "model2 = tree.DecisionTreeClassifier(random_state=1)\n",
    "model = VotingClassifier(estimators=[('lr', model1), ('dt', model2)], voting='hard')\n",
    "model.fit(x_train,y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking\n",
    "def Stacking(model,train,y,test,n_fold):\n",
    "    folds=StratifiedKFold(n_splits=n_fold,random_state=1)\n",
    "    test_pred=np.empty((test.shape[0],1),float)\n",
    "    train_pred=np.empty((0,1),float)\n",
    "    for train_indices,val_indices in folds.split(train,y.values):\n",
    "        x_train,x_val=train.iloc[train_indices],train.iloc[val_indices]\n",
    "        y_train,y_val=y.iloc[train_indices],y.iloc[val_indices]\n",
    "\n",
    "        model.fit(X=x_train,y=y_train)\n",
    "        train_pred=np.append(train_pred,model.predict(x_val))\n",
    "        test_pred=np.append(test_pred,model.predict(test))\n",
    "    return test_pred.reshape(-1,1),train_pred\n",
    "\n",
    "model1 = tree.DecisionTreeClassifier(random_state=1)\n",
    "test_pred1 ,train_pred1=Stacking(model=model1,n_fold=10, train=x_train,test=x_test,y=y_train)\n",
    "train_pred1=pd.DataFrame(train_pred1)\n",
    "test_pred1=pd.DataFrame(test_pred1)\n",
    "\n",
    "model2 = KNeighborsClassifier()\n",
    "test_pred2 ,train_pred2=Stacking(model=model2,n_fold=10,train=x_train,test=x_test,y=y_train)\n",
    "train_pred2=pd.DataFrame(train_pred2)\n",
    "test_pred2=pd.DataFrame(test_pred2)\n",
    "\n",
    "df = pd.concat([train_pred1, train_pred2], axis=1)\n",
    "df_test = pd.concat([test_pred1, test_pred2], axis=1)\n",
    "model = LogisticRegression(random_state=1)\n",
    "model.fit(df,y_train)\n",
    "model.score(df_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging\n",
    "# Na real que a gente só usa RandomForest mesmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting\n",
    "# Usamos amplamente o XGBoost. Para isso, usamos a biblioteca xgb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
