{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceitos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redes neurais são estruturas tecnológicas de machine learning que baseiam seu funcionamento na forma de aprendizagem dos cérebros humanos. Essas rede aprendem com a experiência para obter e melhorar performances de execução. O aprendizado humano é possível graças aos neurônios de nosso cérebro.\n",
    "<br>\n",
    "Os neurônios possuem a seguinte estrutura:\n",
    "- canais de inputs, que são os dentritos; \n",
    "- um centro de processamento desses inputs, o corpo celular; \n",
    "- uma via de propagação de sinal elétrico, os axônios;\n",
    "- canais de output, terminais do axônio.\n",
    "<br>\n",
    "<img src=\"img/neuron.png\" align=\"center\" width=\"50%\">\n",
    "<br>\n",
    "O modelo mais básico de neurônio, desenvolvido por  Frank Rosenblatt, é chamado de **Perceptron**, e possível verificar que sua arquitetura é a mesma de um neurônio humano:\n",
    "<br>\n",
    "<img src=\"img/perceptron.jpg\" align=\"center\" width=\"50%\">\n",
    "<br>\n",
    "- os canais de input xi, que são somados de acordo com um conjunto de pesos wi. \n",
    "- uma **função de ativação** no corpo celular (step function) e o output. \n",
    "<br>\n",
    "A esse movimento dos inputs, sendo multiplicados por seus pesos e passando pela função de ativação como uma combinação linear é dado o nome de movimento **feedfoward**. O machine learning por trás de um perceptron torna-se, então, um problema de otimização dos pesos wi.\n",
    "\n",
    " Ler mais sobre a história das redes neurais __[nesse link](http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning/)__ \n",
    " e seu funcionamento __[nesse outro link](http://neuralnetworksanddeeplearning.com/chap1.html)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É necessário entender como escolher os pesos wi certos para que o output seja correto. Entender como que a variação de cada peso wi afetará o output, além de  atualizar os pesos de acordo com as conclusões. Se há uma função custo, uma **loss function** definida (RMSE, MAE etc), pode-se, então, aplicar um algoritmo já bem conhecido em machine learning: o gradiente descendente! Ao movimento de volta do sinal, comparando os outputs y com as predições yhat e atualizando os pesos de cada conexão input de neurônios, recebe o nome de **backpropagation**. <br>\n",
    "<img src=\"img/neural_net_inner_working.gif\" align=\"right\" width=\"50%\">\n",
    "\n",
    "Aqui pode-se ver o funcionamento de uma rede neural genérica. Repare na ordem dos acontecimentos. Um input entra e realiza o seu movimento de feedfoward. Resultados são gerados e comparados com os valores reais. Então, o sinal faz o caminho contrário na rede, atualizando os valores dos pesos num movimento de backpropagation. A cada ciclo desses, damos o nome de **epoch**.\n",
    "Aina não é uma rede pois existe apenas um neurônio. Mas a ideia é adicionar mais, camadas, mais neurônios, mais funções de ativação. Só a partir dessa complexidade uma rede pode ser chamada de rede neural. O funcionamento dessa rede sempre sempre igual: definir o número de epochs, sua arquitetura e repetir o ciclo de feedfoward-backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma arquitetura de redes neurais pode ser descrita como neurônios diferentes (ou seja, com estrutura e funções de ativação diferentes) realizando uma função diferente na rede. \n",
    "<br>\n",
    "<img src=\"img/neural_net_zoo.png\" align=\"left\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma rede neural pode ser descrita como um aproximado universal de mappings, ou funções, já que ela é um aproximador universal. Uma rede neural consegue naturalmente aproximar funções mesmo que não lineares, segundo o __[Teorema da Aproximação Universal](https://en.wikipedia.org/wiki/Universal_approximation_theorem)__. Pode-se conferir sua prova __[aqui](https://hackernoon.com/illustrative-proof-of-universal-approximation-theorem-5845c02822f6)__. \n",
    "<br>\n",
    "Machine learning é descobrir o mapping interno de um dataset. Ao entender que redes neurais são aproximadores universais de funções, pode-se entender porque elas são tão poderosas, principalmente em casos complexos e não lineares, como visão computacional e NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neurais e seus Parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T14:07:56.661555Z",
     "start_time": "2019-07-18T14:07:56.614680Z"
    }
   },
   "source": [
    "Exercício 0: entre __[nesse playground](https://playground.tensorflow.org/)__ e comece a variar os hiperparâmetros disponíveis. Experimente trocar de dataset, colocar mais neurônios, menos neurônios, mudar camadas, learning rate, tudo. Quais conclusões podemos chegar? Para ajudar vocês a entender os hiperparâmetros de uma rede neural, é bom lembrar os três princípios básicos de uma rede neural: **dados** (o que estou aprendendo), **arquitetura** (quem está aprendendo) e **otimizador** (como está aprendendo). No caso desse playground, podemos mexer nos 2 primeiros itens e um pouco no terceiro ao regular a learning rate. \n",
    "<br>\n",
    "Ao construir uma rede neural, podemos escolher:\n",
    " - **Modelo**: sequencial, recorrente etc. É o tipo geral de rede que estamos criando. Por enquanto, veremos as redes sequencias (fully connected)\n",
    " - **Layers**: quantas, quais e em qual ordem de camadas teremos em nossa arquitetura. Encare isso como blocos de lego que são encaixados para a construção do modelo. Quanto mais blocos, maior a capacidade do modelo de aprender, mas também maior sua chance de overfitting. Existe uma tonelada de tipos de layers que se pode colocar em uma rede.\n",
    " - **Neurônios ou Activation Functions**: para cada camada (ou até cada neurônio), qual será sua função interna. Sigmoide, ReLU, Leaky ReLu, step etc... É importante que sua diferenciação esteja definida sempre, pois queremos que o gradiente descendente funcione. Na dúvida, use ReLU.\n",
    " - **Otimizador**: Batch Gradient Descent, SGD, Mini Batch GD, Nesterov Momentum, Adagrad, Adam, qual otimizador? Na dúvida, use Adam\n",
    " - **Regularização**: terá regularização e com qual peso. Isso ajuda a prever overfitting\n",
    " - **Dropout**: terá ou não esquecimento temporário de neurônios para mais robustez. Isso ajuda a prever overfitting.\n",
    " - **Noise**: quanto de ruído será produzido internamente na rede. Isso ajuda a torná-la mais robusta.\n",
    " - **Epochs**: quantas epochs a rede realizará. Também é possível realizar early-stopping caso a rede alcance a performance desejada\n",
    " - **Loss Function**: qual será a função custo que norteará a otimização do seu modelo\n",
    "<br>\n",
    "Existem mais alguns hiperparâmetros, mas por enquanto isso está suficiente. Vamos então construir nossa primeira rede neural com TF 2.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uma Rede Neural em TF 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando o TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T14:16:27.279839Z",
     "start_time": "2019-07-18T14:16:23.664451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow version 2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "# E checar qual versão estamos utilizando.\n",
    "# Escolhemos a 2.0 pois sua sintaxe e funcionamento sem session.run() e \n",
    "# construção com base em Keras o torna muito mais intuitivo\n",
    "print(\"Using TensorFlow version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando um dataset residente do TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T14:18:08.213954Z",
     "start_time": "2019-07-18T14:18:08.198327Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**O MNIST do TF 2.0 já vem quebrado em treino e test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T14:19:10.681230Z",
     "start_time": "2019-07-18T14:19:10.180807Z"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalizando os valores de cada pixel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T14:19:36.636755Z",
     "start_time": "2019-07-18T14:19:36.292821Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montando a rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciando o modelo sequencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T14:20:40.496947Z",
     "start_time": "2019-07-18T14:20:37.924817Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)), # camada para transformar a imagem 28x28 em um vetor unidimensional\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu), # 512 neuronios fully connected com ativação por ReLU\n",
    "  tf.keras.layers.Dropout(0.2), # Dropout de 20% da última camada\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax) # 10 neurônios softmax, para classificação. Um para cada categoria\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo Loss Function e Otimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T14:21:20.435477Z",
     "start_time": "2019-07-18T14:21:20.294858Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', # Estamos utilizando o ADAM\n",
    "              loss='sparse_categorical_crossentropy', # nossa loss function é entropia cross-categórica\n",
    "              metrics=['accuracy']) # nossa métrica base será acc. Podemos utilizar mais de uma métrica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizando o .fit do modelo com 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T14:22:42.118827Z",
     "start_time": "2019-07-18T14:22:00.744480Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0718 11:22:00.994471  7392 deprecation.py:323] From C:\\Users\\Marcos\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 10s 169us/sample - loss: 0.2204 - accuracy: 0.9350\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 8s 125us/sample - loss: 0.0970 - accuracy: 0.9704\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.0694 - accuracy: 0.9787\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.0538 - accuracy: 0.9829\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.0430 - accuracy: 0.9862\n",
      "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0662 - accuracy: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06616890939560253, 0.98]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com poucas linhas de código, chegamos numa ótima acurácia. Repare na evolução da loss e accuracy ao longo das epochs. O que pode ser observado sobre sua variação e velocidade de variação? Qual a causa disso? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
